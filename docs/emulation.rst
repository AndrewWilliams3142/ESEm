
===================
Emulating with ESEm
===================

ESEm provides a simple and streamlined interface to emulate earth system datasets stored as iris Cubes, denoted :math:`Y` in the following documentation.
The corresponding predictors (:math:`X`) can be provided as a numpy array or pandas `DataFrame`s.
This emulation is essentially just a regression estimating the functional form:

.. math::

   Y \approx f(X)

and can be performed using a variety of techniques using the same API.


Gaussian processes emulation
============================

Gaussian processes (GPs) are a popular choice for model emulation due to their simple formulation and robust uncertainty estimates, particularly in cases of relatively small amounts of training data.
Many excellent texts are available to describe their implementation and use (Rasmussen and Williams, 2005) and we only provide a short description here.
Briefly, a GP is a stochastic process (a distribution of continuous functions) and can be thought of as an infinite dimensional normal distribution (hence the name).

The ESEm GP emulation module provides a thin wrapper around the `GPFlow <https://gpflow.readthedocs.io/en/master/#>`_ implementation.
Please see their documentation for a detailed description of the implementation.

An important consideration when using RP regression is the form of the covariance matrix, or kernel. Typical kernels include: constant; linear; radial basis function (RBF; or squared exponential); and Matérn 3/2 and 5/2 which are only once and twice differentiable respectively.
Kernels can also be designed to represent any aspect of the functions of interest such as non-stationarity or periodicity.
This choice can often be informed by the physical setting and provides greater control and interpretability of the resulting model compared to e.g., Neural Networks.

.. Note::
    By default, ESEm uses a combination of linear, RBF and polynomial kernels which are suitable for the smooth and continuous parameter response expected for the examples used in this paper and related problems.
    However, given the importance of the kernel for determining the form of the functions generated by the GP we have also included the ability for users to specify combinations of other common kernels.
    See e.g., `Duvenaud, 2011 <https://www.cs.toronto.edu/~duvenaud/thesis.pdf>`_ for a clear description of some common kernels and their combinations, as well as work towards automated methods for choosing them.

The framework provided by GPFlow also allows for multi-output GP regression and ESEm takes advantage of this to automatically provide regression over each of the output features provided in the training data.
E.g. :math:`Y` can be of arbitrary dimensionality. It will be automatically flattened and reshaped before being passed to GPFlow.

.. code-block:: python

    from esem import gp_model



Neural network emulation
========================

Through the development of automatic differentiation and batch-gradient descent it has become possible to efficiently train very large (millions of parameters), deep (dozens of layers) neural networks, using large amounts (terabytes) of training data.
The price of this scalability is the risk of overfitting, and the lack of any information about the uncertainty of the outputs.
However, both of these shortcomings can be addressed using a technique known as ‘dropout’ whereby individual weights are randomly set to zero and effectively ‘dropped’ from the network.
During training this has the effect of forcing the network to learn redundant representations and reduce the risk of overfitting (Srivastava et al., 2014).
More recently it was shown that applying the same technique during inference casts the NN as approximating Bayesian inference in deep Gaussian processes and can provide a well calibrated uncertainty estimate on the outputs (Gal and Ghahramani, 2015).
The convolutional layers within these networks also take into account spatial correlations which cannot currently be directly modelled by GPs (although dimension reduction in the input can have the same effect).
The main drawback with a CNN based emulator is that they typically need a much larger amount of training data than GP based emulators.

While fully connected neural networks have been used for many years, even in climate science (Knutti et al., 2006; Krasnopolsky et al., 2005), the recent surge in popularity has been powered by the increases in expressibility provided by deep, convolutional neural networks (CNNs) and the regularisation techniques which prevent these huge models from over-fitting the large amounts of training data required to train them.
Many excellent introductions can be found elsewhere but, briefly, a neural network consists of a network of nodes connecting (through a variety of architectures) the inputs to the target outputs via a series of weighted activation functions.
The network architecture and activation functions are typically chosen a-priori and then the model weights are determined through a combination of back-propagation and (batch) gradient descent until the outputs match (defined by a given loss function) the provided training data. As previously discussed, the random dropping of nodes (by setting the weights to zero), termed dropout, can provide estimates of the prediction uncertainty of such networks.
The computational efficiency of such networks and the rich variety of architectures available have made them the tool of choice in many machine learning settings, and they are starting to be used in climate sciences for emulation (Dagon et al., 2020), although the large amounts of training data required have so far limited their use somewhat.

.. image:: images/CNN_diagram.png
  :width: 400
  :alt: Schematic illustration of the structure of the default convolutional neural network model.


Random forest emulation
=======================

ESEm also provides the option for emulation with Random Forests using the open-source implementation provided by scikit-learn.
Random Forest estimators are comprised of an ensemble of decision trees; each decision tree is a recursive binary partition over the training data and the predictions are an average over the predictions of the decision trees (Breiman, 2001).
As a result of this architecture, Random Forests (along with other algorithms built on decision trees) have two main attractions.
Firstly, they require very little pre-processing of the inputs as the binary partitions are invariant to monotonic rescaling of the training data.
Secondly, and of particular importance for climate problems, they are unable to extrapolate outside of their training data because the predictions are averages over subsets of the training dataset.
As a result of this, a Random Forest trained on output from an idealized GCM was shown to automatically conserve water and energy (O’Gorman and Dwyer, 2018).

These features are of particular importance for problems involving the parameterization of sub-grid processes in climate models (Beucler et al., 2021) and as such, although parameterization is not the purpose of ESEm, we include a simple Random Forest implementation and hope to build on this in future.


Data processing
===============

Many of the above approaches make assumptions, or simply perform better, when the training data is structured or distributed in a certain way.
These transformations are purely to help the emulator fit the training data, and can complicate comparison with e.g. observations during calibration.
ESEm provides a simple and transparent way of transforming the datasets for training, and this automatically un-transforms the model predictions to aid in observational comparison.

Where these transformations are strictly necessary for a given model then it will be included in the wrapper function. Other choices are left to the user to apply as required.

A full list of the data processors can be found in the API documentation: `api`_

Feature selection
=================

ESEm includes a simple utility function that wraps the scikit-learn LassoLarsIC regression tool in order to enable an
initial feature (parameter) selection. This can be useful to reduce the dimensionality of the input space. Either the
Akaike information criterion (AIC) or the Bayes Information criterion (BIC) can be used, although BIC is the default.

For example,

.. code-block:: python

    from esem import gp_model
    from esem.utils import get_param_mask

    # X and Y are our model parameters and outputs respectively.
    active_params = get_param_mask(X, y)

    # The model parameters can then be subsampled either directly
    X_sub = X[:, active_params]

    # Or by specifying the GP active_dims
    active_dims, = np.where(active_params)
    model = gp_model(X, y, active_dims=active_dims)


Note, this estimate only applies to one-dimensional outputs. Feature selection for higher dimension outputs is a much
harder task beyond the scope of this package.
